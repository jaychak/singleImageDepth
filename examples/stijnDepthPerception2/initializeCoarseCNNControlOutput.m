function net = initializeCoarseCNNControlOutput(varargin)
 %% Modified Stijn's network for control output
 %% Jay Chakravarty 
 %% Aug 2016
 
num_control_outputs = 2;
opts.useBatchNorm = true ; %%  CHANGE THIS BACK TO TRUE
opts.networkType = 'simplenn' ;
opts = vl_argparse(opts, varargin) ;

f=0.005; % Initialiaze filters on random values times f
lr = ones(2,1); % Give learning rate of filters
% c1 = 0.00053; % Constant to change learning rate 1
c1=0.001;
% c2 = 0.06; % Constant to change learning rate 2
c2 = 0.1;
leak = 0; % Leak factor for leaky ReLU (0 by default)

rng('default');
rng(0) ;

net.layers = {} ;
% Coarse 1
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(11,11,3,96, 'single'),zeros(1, 96, 'single')}}, ...
                           'learningRate', c1*lr, ...
                           'stride', 4, ...
                           'pad', 0) ;%Layer 1
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;%Layer 2
net.layers{end+1} = struct('type', 'relu', 'leak', leak) ;%Layer 3
% Coarse 2
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(5,5,96,256, 'single'),zeros(1,256,'single')}},...
                           'learningRate', c1*lr, ...
                           'stride', 1, ...
                           'pad', 2) ;%Layer 4
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;%Layer 5
net.layers{end+1} = struct('type', 'relu', 'leak', leak) ;%Layer 6
% Coarse 3
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(3,3,256,384, 'single'),zeros(1,384,'single')}},...
                           'learningRate', c1*lr, ...
                           'stride', 1, ...
                           'pad', 1) ;%Layer 7
net.layers{end+1} = struct('type', 'relu', 'leak', leak) ;%Layer 8
% Coarse 4
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(3,3,384,384, 'single'),zeros(1,384,'single')}},...
                           'learningRate', c1*lr, ...
                           'stride', 1, ...
                           'pad', 1) ;%Layer 9
net.layers{end+1} = struct('type', 'relu', 'leak', leak) ;%Layer 10
% Coarse 5
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(3,3,384,256, 'single'),zeros(1,256,'single')}},...
                           'learningRate', c1*lr, ...
                           'stride', 2, ...
                           'pad', 0) ;%Layer 11
net.layers{end+1} = struct('type', 'relu', 'leak', leak) ;%Layer 12
% Coarse 6 - fully connected
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(6,9,256,4096, 'single'),zeros(1,4096,'single')}},... % Filter should be as large as previous feature map!
                           'learningRate', c2*lr, ...
                           'stride', 1, ...
                           'pad', 0) ;%Layer 13
net.layers{end+1} = struct('type', 'relu', 'leak', leak) ;%Layer 14
net.layers{end+1} = struct('type', 'dropout', ...
                           'rate', 0.7) ;%Layer 15
% Coarse 7 - fully connected
% net.layers{end+1} = struct('type', 'conv', ...
%                            'weights', {{f*randn(1,1,4096,4070, 'single'),zeros(1,4070,'single')}},... 
%                            'learningRate', c2*lr, ...
%                            'stride', 1, ...
%                            'pad', 0, ...
%                            'rememberOutput', 1) ;%Layer 16
% net.layers{end+1} = struct('type', 'bnorm', ...
%                'weights', {{ones(4070, 1, 'single'), zeros(4070, 1, 'single')}}, ...
%                'learningRate', [1 1 0.05], ...
%                'weightDecay', [0 0], ...
%                 'rememberOutput', 1) ;%Layer 17

%% Control output
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(1,1,4096,num_control_outputs, 'single'),zeros(1,num_control_outputs,'single')}},... 
                           'learningRate', c2*lr, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'rememberOutput', 1) ;%Layer 16
net.layers{end+1} = struct('type', 'bnorm', ...
               'weights', {{ones(num_control_outputs, 1, 'single'), zeros(num_control_outputs, 1, 'single')}}, ...
               'learningRate', [1 1 0.05], ...
               'weightDecay', [0 0], ...
                'rememberOutput', 1) ;%Layer 17

            
net.layers{end+1} = struct('type', 'lossEigen', ...
                           'rememberOutput', 1);

% optionally switch to batch normalization
if opts.useBatchNorm
  net = insertBnorm(net, 1) ;
  net = insertBnorm(net, 5) ;
  net = insertBnorm(net, 9) ;
  net = insertBnorm(net, 12) ;
  net = insertBnorm(net, 15) ;
  net = insertBnorm(net, 18) ;
end

% Fill in default values
net = vl_simplenn_tidy(net) ;
end

% --------------------------------------------------------------------
function net = insertBnorm(net, l)
% --------------------------------------------------------------------
assert(isfield(net.layers{l}, 'weights'));
ndim = size(net.layers{l}.weights{1}, 4);

layer = struct('type', 'bnorm', ...
               'weights', {{ones(ndim, 1, 'single'), zeros(ndim, 1, 'single')}}, ...
               'learningRate', [1 1 0.05], ...
               'weightDecay', [0 0]) ;

net.layers{l}.biases = [] ;
net.layers = horzcat(net.layers(1:l), layer, net.layers(l+1:end)) ;
end
